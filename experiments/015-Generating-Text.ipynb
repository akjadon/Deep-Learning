{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge \n",
    "\n",
    "A corpus of manually labeled comments - classifying each comment by its degree of toxicity is available on Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "! wget http://bit.do/deep_toxic_train -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv data/deep_toxic_train data/train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data && unzip train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the train and label datasets (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.iloc[:,2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the train data\n",
    "\n",
    "- **Tokenization**\n",
    "\"I am going to school\" -> [\"I\", \"am\", \"going\", \"to\", \"school\"]\n",
    "\n",
    "- **Indexing**\n",
    "{1:\"I\", 2:\"am\", 3:\"going\", 4:\"to\", 5:\"school\" }\n",
    "\n",
    "- **Index Representation**\n",
    "[1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(train_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding count of occurences of a word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3183"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts[\"hey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding index of a word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"hey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index representation of a sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sentence\n",
    "train[\"comment_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[688,\n",
       " 75,\n",
       " 1,\n",
       " 126,\n",
       " 130,\n",
       " 177,\n",
       " 29,\n",
       " 672,\n",
       " 1116,\n",
       " 86,\n",
       " 331,\n",
       " 51,\n",
       " 50,\n",
       " 15,\n",
       " 60,\n",
       " 148,\n",
       " 7,\n",
       " 34,\n",
       " 117,\n",
       " 1221,\n",
       " 4,\n",
       " 45,\n",
       " 59,\n",
       " 244,\n",
       " 1,\n",
       " 365,\n",
       " 31,\n",
       " 1,\n",
       " 38,\n",
       " 27,\n",
       " 143,\n",
       " 73,\n",
       " 89,\n",
       " 985]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index representation\n",
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find index of the first word: explanation\n",
    "tokenizer.word_index[\"explanation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding**\n",
    "How to handle variable length sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenized_train, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did we select 200?**\n",
    "\n",
    "find length of each sentence and plot the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = [len(comment) for comment in tokenized_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(number_of_words, bins = np.arange(0, 500, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(max_features, output_dim=embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      " 56592/143613 [==========>...................] - ETA: 41:05 - loss: 0.0754 - acc: 0.9762"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y, batch_size=16, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2**\n",
    "\n",
    "Using keras functional API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer. We will create embedding as part of the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Embedding(max_features, embedding_size)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences=True, name = \"lstm_layer\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(50, activation = \"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = inp, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"binary_crossentropy\",\n",
    "             optimizer = \"adam\", \n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y, batch_size=batch_size, epochs = epochs, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://bit.do/pydeeptexthelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv pydeeptexthelper text_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[train[\"threat\"]==1][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)}\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Keras, the input for the network should be of the form:\n",
    "(number_of_sequences, length_of_sequence, number_of_features)\n",
    "\n",
    "length_of_sequence -> SEQ_LENGTH defined above (50). \n",
    "number_of_features -> the length of the char array above (478)\n",
    "number_of_sequences -> length of data / length of each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((int(len(data)/SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "y = np.zeros((int(len(data)/SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\n",
    "for i in range(0, int(len(data)/SEQ_LENGTH)):\n",
    "    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 0\n",
    "while nb_epoch < 10:\n",
    "    print('\\n\\n')\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH)\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
